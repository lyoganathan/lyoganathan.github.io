---
layout: single
title: "Hypothesis Testing With Regression"
excerpt: "Comparing regression and t-test when you have a continous Y and a categorical X"
date: "2021-09-01"
tags: [regression, categorical, hypothesis, testing, multiple, categories]
values:
  show_date: true
toc: yes
---

```{r include=FALSE}
knitr::opts_knit$set(base.dir = "C:/Users/Laagi/Documents/GitHub/lyoganathan.github.io", base.url = "/")
knitr::opts_chunk$set(
  fig.path = "assets/images/regression_hypothesis_testing/",
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  dev.args=list(bg = "transparent")
)
```

## Intro

There's many different tests avaliable to do hypothesis testing a.k.a A/B testing. Permutation test, t-test, chi-square test, [bayesian estimation](https://jkkweb.sitehost.iu.edu/articles/Kruschke2013JEPG.pdf), [GEE](https://quoradata.quora.com/A-Robust-Statistical-Test-for-Ratio-Metrics)...

This posts goes through regression with groups and shows how to get the same result as a t-test, and why regression works for hypothesis testing. I will also try to link this to the p-values and t-values of the regression coefficients.


## Palmer penguins dataset

I've been watching a lot of Atypical recently and found out there's a dataset on penguins in Antarctica available in the _palmerpenguins_ library:

```{r}
library(palmerpenguins)
library(ggplot2)
library(scatterplot3d)
library(knitr)

kable(head(penguins,10))
```

### Visualize body mass of species
```{r}
# Remove NAs
penguins_df = na.omit(penguins)

# Histogram, position='identity' makes sure its not stacked
#ggplot(penguins_df, aes(x=body_mass_g, fill=species, color=species)) + geom_histogram(alpha=0.4, position='identity')

# Density plot:
ggplot(penguins_df, aes(x=body_mass_g, fill=species)) + geom_density(alpha=0.8) + theme_minimal()
```

### Summary Statistics
```{r}
# Mean body mass for each species
aggregate(body_mass_g ~ species, penguins_df, mean)

# Number of data points for each species
aggregate(body_mass_g ~ species, penguins_df, length)
```


### Adelie vs Chinstrap

Let's test the difference between body mass of Adelie and Chinstrap using regression and compare the result to a t-test. First let's look at the absolute difference:

```{r}
# Here we keep rows that contain Adelie or Chinstrap and only keep species and body_mass_g column
test_df = penguins_df[penguins_df$species %in% c('Adelie','Chinstrap'), c("species","body_mass_g")]

# Create factors again to remove unused factor level:
test_df$species = factor(test_df$species)

# Difference between means:
diff(by(test_df$body_mass_g, test_df$species, mean))
```

## Regression with categorical variables

Our x variable (species) is a category/factor. In python, you might have to use one hot encoding on your categorical variables before you pass your data into a linear model. The nice thing about R is that it does the one hot encoding/dummy coding on factors automagically for us:

```{r}
two_lvl_model = lm(body_mass_g ~ species, data = test_df)
summary(two_lvl_model)
```

### Dummy Coding / One-Hot Encoding

Let's manually do the dummy coding and compare results. If you think about a linear model $y=mx+b$ where $x$ is species and $y$ is body mass, we need $x$ to be $0$ for one category and $1$ for the other. In this case let's make $Chinstrap$ $1$ and $Adelie$ $0$.

```{r}
# Manually create dummy variables:
test_df$x = ifelse(test_df$species == 'Chinstrap',1,0)
```

Now we do regression with our new variable $x$.
```{r}
two_lvl_dummy = lm(body_mass_g ~ x, data = test_df)
summary(two_lvl_dummy)
```

We see that we got the exact same result as before.

### Intrepreting Regression Coefficients

```{r}
# Intercept:
two_lvl_model$coefficients[1]

# Slope
two_lvl_model$coefficients[2]
```

Do these numbers look familar? The slope is $26.92$ which is actually the group difference. The 
intercept is $3706.16$ which is mean of Adelie group. Why is this the case?

Let's go back to the equation for this linear model, keeping in mind the dummy coding of our categorical variable. Adelie is represented by $x = 0$ and Chinstrap is $x = 1$.

If we sub in $x=0$, we will get $y=3706.164$ which is the mean of Adelie body mass:

$$ y = 26.92(0) + 3706.164 = 3706.164 $$

When $x = 1$, $y=3733.088$ which is the mean of Chinstrap body mass:

$$ y = 26.92(1) + 3706.164 = 3733.088 $$

But why is the slope the mean difference? Why is it that the line passes through the mean of both groups?

Let's plot the situation:

```{r}
plot.default(x=test_df$species, y=test_df$body_mass_g)
lines(test_df$species, predict(two_lvl_model),col='blue')
```

The way the slope and intercept are calculated is called Oridnary Least Squares. It minimizes the sum of squared deviations. For every x variable our model predicts a value $\hat{y_i}$. Our $x$ variable is categorical with 2 levels: $Adelie(0)$ or $Chinstrap(1)$. 

$$\sum_{i=1}^{n} (y_i - \hat{y_i})^{2}$$

The least squares formula works by minimizing the sum of squared deviations. In this case the mean of both groups is what minimizes the sum of squared errors.

Let's take a simple example with 3 data points in one dimension. Let's say each data point is $y$. What value of $\hat{y}$ would minimize the sum of squared errors (the formula above)?.

![Residuals in one dimension](/assets/images/lregression_hypothesis_testing/sse_1d.png)

The mean is $0.6$ which is the point that minimizes sum of squared deviations. You can try with any other point, but you will find that using the mean as $\hat{y}$ minimizes the above equation. You may also notice that the median will minimize the sum of absolute deviations.

So, the line of best fit passes through both group means because the group means minimize the squared errors.

### Interpreting t-value & p-value of the slope

What does the t-value represent here? Why does regression have a t-value at all?
Where is null t distribution and the area under the curve?

It is essentially a one sample t-test. The t-value and p-value are from a one sample t-test on the regression coefficient. And as we saw above, in this case the slope *_is_* the mean difference. So we are in a sense testing if the difference in means is different from $0$. If we compare the p-value of the slope here to a t-test or permutation test, we should see similar results. In fact, it should be identical to the t-test that uses pooled variance.

The statistical test which is conducted for the statistical significance of the coefficient is a one sample t-test. This is confusing since we do not have a "sample" of multiple coefficients for X4, but we have an estimate of the distributional properties of such a sample using the central limit theorem. The mean and standard error describe the location and shape of such a limiting distribution. If you take the column "Est" and divide by "SE" that gives you the t-value. If you use the t-value and look it up on a t-distribution with the given degrees of freedom, this gives you the p-values.

We can also compare this result to what we would get from a t-test:

## Regression vs t-test

```{r}
# If you set var.equal = T it will give the same t-value & p-value as the regression
t.test(test_df$body_mass_g ~ test_df$species, var.equal = T)
```

### Why is the p-value for t-test and regression exactly the same?

T-test forumla:

$$ t = \frac{\bar{x_1} - \bar{x_2}}{ \sqrt{s_p^2 (\frac{1}{n_1} + \frac{1}{n_2}) }} $$

The denominator is the pooled standard error and $s_p^2$ is the pooled variance which is:

$$ s_p^2 = \frac{\sum_{i=1}^{n_1}(x_i - \bar{x_1}) + \sum_{j=1}^{n_2}(x_j - \bar{x_2})}{n_1 + n_2 - 2} $$

Which can also be written as (see [here](https://stackoverflow.com/a/21385702)):

$$ s_p^2 = \frac{(n_1 - 1)s_1^2 + n_2 - 1 (s_2^2)}{n_1 + n_2 - 2} $$

Let's calculate the t-value manually:
```{r}
# Variacne estimator of coefficients from linear model that meets assumptions:
# In this case Xs are just 0 or 1, Y is the body mass for respective species

var1 = var(test_df$body_mass_g[test_df$species == "Chinstrap"])
var2 = var(test_df$body_mass_g[test_df$species == "Adelie"])

n1 = length(test_df$body_mass_g[test_df$species == "Chinstrap"])
n2 = length(test_df$body_mass_g[test_df$species == "Adelie"])

# Pooled variance: 
var_pool = ( (n1-1)*var1 + (n2-1) * var2) / (n1+n2-2)

# Pooled standard error:
se_pool = sqrt( var_pool * (1/n1 + 1/n2) )
```

### Regression t-value

Variance of the slope (sigma is variance of the error term):
$$ \frac{\sigma^{2}}{\sum(X_i - \bar{X})^{2}} $$

Here is a [video](https://www.youtube.com/watch?v=rODUBTRUV0U&ab_channel=jbstatistics) showing the derivation for variance:

And a [stack post](https://stats.stackexchange.com/questions/85943/how-to-derive-the-standard-error-of-linear-regression-coefficient) explaining standard error: 


You can kind of think of SE of slope in a similar manner to to SE of any data. The SE of data is when you repeatedly sample the data and get the mean. This distribution is called the sampling distribution of the mean and the SD of this distribution is the SE. Central limit theorm tells us the SE is $\frac{\sigma}{\sqrt{n}}$.

Similarly for slope, imagine if you repeatedly sampled your data and calculated the slope of the best fit line. If the residuals are small, we might except slope to have a narrower range of values. If the residuals are large we are less sure of the true value of the slope, and thus might have a larger range of values.

```{r}
# SE of the slope is the same as SE pooled
sqrt(sum((two_lvl_model$residuals - mean(two_lvl_model$residuals))^2) / (length( two_lvl_model$residuals) - 2)) / sqrt(sum( (test_df$x - mean(test_df$x) )^2 ))
```

The regression t-value is calculated using estimates of the mean difference and standard error obtained mathematically.

We see how the standard error of both are the same. We also know the numerator is the same. In the t-test it is the mean difference, and in regression it is the slope, which is equal to the mean difference in this case.

## Regression with a factor with 3 levels

```{r}
three_lvl_model = lm(body_mass_g ~ species, data = penguins_df)
summary(three_lvl_model)
```

### The equivalant is one way anova and tukey HSD t-tests

```{r}
three_lvl_aov = aov(body_mass_g ~ species, data = penguins_df)
summary(three_lvl_aov)
```
TukeyHSD gives us an additional piece of information: the Gentoo-Chinstrap relationship which is not present in our regression above.

```{r}
TukeyHSD(three_lvl_aov)
```

Now let's visualize this:

### Plot data
```{r}
plot.default(x=penguins_df$species,y=penguins_df$body_mass_g)
lines(penguins_df$species, predict(three_lvl_model),col='blue')
```

Well this is pretty wild, are we still looking at linear regression? The lines are disjoint and broken. What could be going on here? However, note that it does manage to pass through the means of all three groups (based on regression coefficients) which means it does minimize the least squares.

Let's go back to the regression equation. In the regression output above we had an intercept and 2 slopes, meaning there was 2 $x$ variables. This is because inorder to represent 3 levels of a categorical variable, we need 2 dummy variables. So the one hot encoding R is doing will be something like this:

$$ y = m_1x_1 + m_2x_2 + b $$

Where different combinations of $x_1$ & $x_2$ will be different categories. Adelie would be $x_1 = 0, x_2 = 0$, Chinstrap would be $x_1 = 1, x_2 = 0$ and Gentoo would be $x_1 = 0, x_2 = 1 $. So, this is actually regression with 3 dimensions, but our plot was only 2 dimensions. [This post expains the concept beautifully.]( https://stats.stackexchange.com/questions/92065/why-is-polynomial-regression-considered-a-special-case-of-multiple-linear-regres)

We can manually create this coding and verify we get the exact same result:

```{r}
# Manually create dummy variables:
penguins_df$x1 = ifelse(penguins_df$species == 'Chinstrap',1,0)
penguins_df$x2 = ifelse(penguins_df$species == 'Gentoo',1,0)

penguins_df
```

```{r}
# Double check that this is the same as the model we did before with UDF_text_07
three_lvl_dummy = lm(body_mass_g ~ x1+x2, data = penguins_df)
summary(three_lvl_dummy)
```


We get the exact same result as above.

We can visualize the 3D regression to get an idea of where the broken lines come from:
```{r}
s3d = scatterplot3d(z=penguins_df$body_mass_g, x=penguins_df$x1, y=penguins_df$x2)
s3d$plane3d(three_lvl_dummy,draw_polygon = TRUE)
```

If you try to mentally squish this into a 2D graph like we saw before, you can kind of see how we got the broken graph earlier.


## References:

https://stats.stackexchange.com/questions/92065/why-is-polynomial-regression-considered-a-special-case-of-multiple-linear-regres


How come we can use linear regression for hypothesis testing:
https://stats.stackexchange.com/questions/128723/understanding-of-p-value-in-multiple-linear-regression

https://stats.stackexchange.com/questions/117406/proof-that-the-coefficients-in-an-ols-model-follow-a-t-distribution-with-n-k-d

https://stats.stackexchange.com/questions/285986/distribution-of-linear-regression-coefficients/285992

https://stats.stackexchange.com/questions/342632/how-to-understand-se-of-regression-slope-equation/342672

https://stats.stackexchange.com/questions/344006/understanding-t-test-for-linear-regression

http://home.cc.umanitoba.ca/~godwinrt/4042/material/part3.pdf